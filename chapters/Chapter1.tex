% !TEX root = ../main.tex
\chapter{绪论}

\section{研究意义与背景}
深度估计作为三维计算机视觉领域的核心基础问题，旨在从二维图像中重建场景的几何特征，在过去十余年中经历了从手工特征建模到深度学习驱动的范式变迁。
随着传感器技术与计算能力的提升，深度估计已成为智能无人系统实现环境感知与定位导航的关键技术：在自动驾驶中，它为障碍物检测与路径规划提供必要的距离信息；
在无人机技术中，它是实现自主避障与三维测绘的基础；在工业机器人领域，深度估计则赋予了机械臂精准的抓取与协作能力。鉴于三维空间信息的不可替代性，
深度估计在未来空间计算与具身智能（Embodied AI）的发展浪潮中，其战略性地位将得到进一步巩固与提升。

然而，现有的深度估计方法在复杂多变的实际场景下仍表现出一定的局限性，其中泛化性能不足的问题尤为突出。具体而言，模型往往在特定的训练数据集（如 KITTI 或 NYU Depth V2）上表现优异，但在面对未见过的光照变化、极端天气或异质场景时，预测精度通常会出现显著下降。
更深层次的挑战在于，深度估计模型往往对相机内参（如焦距、光心位置）具有极强的依赖性。由于单目深度估计本质上是一个病态或不适定（Ill-posed）问题，
即单幅二维图像中的每一个像素点都对应着三维空间中无数种可能的深度解释。目前的深度神经网络往往通过“走捷径”的方式，过度拟合训练数据中特定相机的成像几何特性，
而非真正理解场景的物理尺度。这种对特定成像设备的过度耦合，使得模型在跨相机、跨平台部署时，难以实现稳健的零样本（Zero-shot）迁移。
这种局限性在实际应用中引发了诸多挑战。以无人机自主导航为例，理想的系统应具备跨场景的感知能力，然而现有的深度估计模型往往难以在光室内与室外环境间实现无缝切换。
当无人机从开阔的室外环境飞入结构复杂的室内空间时，由于场景分布与光照特性的剧烈波动，泛化性能较差的模型往往会产生严重的深度偏差，导致定位失效或碰撞风险。
此外，模型对训练数据的过度拟合进一步削弱了其在极端环境下的鲁棒性。 在强光、重雾或暴雨等恶劣天气条件下，
图像的对比度和信噪比大幅下降，现有的深度模型由于缺乏对场景本质几何特征的提取能力，
容易受到环境噪声的干扰，导致深度预测图出现严重的空洞或畸变。这种在非理想成像条件下的性能塌缩，已成为制约深度估计技术走向全天候、全场景应用的核心障碍。