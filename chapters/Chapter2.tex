\chapter{基于相对深度引导的映射模块}
\section{引言}
随着具身智能技术的普及，单目深度估计（MDE）已成为自动驾驶与机器人感知的核心环节。按照表征形式，
MDE 可分为相对深度估计与绝对深度（Metric Depth）估计，后者因能恢复具备真实物理尺度的像素级距离，
成为系统从“视觉感知”迈向“物理交互”的关键桥梁。在避障规划、目标抓取及视觉 SLAM 尺度恢复等任务中，
绝对尺度信息提供了不可或缺的几何度量约束。
相比激光雷达等昂贵设备，单目绝对深度估计凭借低成本与高灵活性，已成为构建全场景无人系统感知的核心基石。

如前文所述，从单幅图像推算绝对深度本质上是一个典型的“不适定问题”（Ill-posed Problem）。 
目前主流的单目绝对深度估计多采用端到端的直接回归范式，这种“一步到位”的映射机制使得模型不可避免地陷入尺度信息与几何结构的强耦合困境。
在这种机制下，模型往往过度依赖训练集中的语义先验（如物体的平均尺寸）来推断距离，而非理解真正的投影几何关系。
这导致模型在面对异质场景或相机内参发生波动时，极易产生剧烈的尺度漂移（Scale Drift），并导致物体边缘与空间布局的一致性受损。

值得注意的是，现有的绝对深度估计工作大多倾向于构建从图像特征到物理数值的直接线性或非线性映射，
这种粗放的回归方式忽略了相对深度中蕴含的稳健几何拓扑约束。由于绝对深度真值（Ground Truth）通常由稀疏的激光雷达获取，
且覆盖场景有限，模型在拟合有限的数值分布时，容易丧失对物体间细粒度几何序关系的把握。这导致模型在处理复杂场景时，
虽然能在统计数值上逼近真值，但其三维空间结构常发生畸变，无法满足高精度导航对环境结构化的严苛要求。

然而，现有的绝对深度估计工作大多倾向于构建从图像到物理数值的直接映射，
这种“一步到位”的粗放回归方式往往忽略了相对深度中蕴含的稳健几何拓扑约束。这导致模型在处理复杂场景时，
虽然能在数值上逼近真值，但其空间结构常发生畸变，且极易受相机参数波动的干扰。

针对上述挑战，本章提出了一种融合相对深度特征先验的二阶段解构式绝对深度估计方法。
该方法的核心逻辑在于将绝对深度估计任务解构为“结构感知”与“尺度适配”两个独立且互补的环节。
第一阶段利用具有强泛化力的预训练基础模型提取尺度不变的相对几何特征；第二阶段则通过一个轻量化映射模块，
利用多尺度特征块（Feature Blocks）中丰富的上下文信息引导模型完成从相对空间到物理尺度的精准转换。
这种解耦设计不仅能最大限度保留基础模型在“野外”环境下的结构泛化力，
更通过特征驱动的映射机制实现了物理尺度的精确对齐，为构建高鲁棒性的绝对深度感知系统提供了新思路。

\section{模型总体结构设计}

% 感觉图片里字有点小了。另外 不知道图片中英文好不好
\begin{figure}[htbp]
    \centering
    \begin{adjustbox}{center} % 强制相对于页面中心对齐
        \includegraphics[width=1.2\textwidth]{figures/主结构-最新drawio.drawio.pdf}
    \end{adjustbox}
    \caption{单目深度估计任务解构与特征提取框架}
    \label{fig:model_arch}
\end{figure}

本章所设计的绝对深度估计模型采用了任务解构的二阶段架构，旨在充分利用预训练基础模型的几何结构泛化能力，
并通过轻量化的度量映射分支实现物理尺度的精准恢复。模型总体架构如图\ref{fig:model_arch}所示，主要由特征提取编码器（Encoder）、
相对深度估计模块（Relative Estimation Module）以及度量深度估计模块（Metric Estimation Module）三部分组成。


\subsection{任务解构与特征提取}
为了从根本上解决单目绝对深度估计中由于投影歧义性导致的尺度与结构强耦合问题，
本文将深度恢复过程解构为“结构感知”与“尺度适配”两个解耦的子任务。
该设计旨在利用大规模预训练模型（如 DPT-BEiT 或 MiDaS 系列）的几何泛化能力，为绝对尺度的恢复提供稳健的拓扑约束。

如图\ref{fig:model_arch}左侧所示，编码器码器模块是整个深度估计框架的特征提取基石。给定输入 RGB 图像$I \in \mathbb{R}^{H \times W \times 3}$，
编码器的主要任务是将高维像素信息映射为包含语义的深层特征表示。
为了同时满足相对深度估计对全局结构的需求，以及度量深度恢复对局部细节的需求，
本文采用基于 Vision Transformer (ViT) \cite{dosovitskiy2021image}的架构（如 DPT/BEiT）作为主干网络。
与传统的卷积神经网络不同，Transformer 架构天然具备长距离建模能力，能够有效捕捉场景中的上下文依赖关系。
编码器在处理过程中生成两类关键的特征表示，分别服务于后续的两个并行分支。

%% 这一段公式和公式说明很多。其实显得有些啰嗦了。而且也不知道会不会影响查重。有必要的话还是删除一部分吧。
1.全局语义标记 (Tokens $\mathbf{T}$)：
如图中上方路径所示，输入图像首先被切分为固定大小的图块（Patches），并被展平为序列向量。
%% 公式及说明-1  删除从这里开始
具体而言，对于分辨率为 $H \times W$ 的输入图像 $I$，我们将切分为 $N = HW/P^2$ 个尺寸为 $P \times P$ 的图块。
通过线性投影映射与位置编码相加，初始化输入序列 $\mathbf{z}_0$：
\begin{equation}
\mathbf{z}_0 = [\mathbf{x}_p^1\mathbf{E}; \mathbf{x}_p^2\mathbf{E}; \cdots; \mathbf{x}_p^N\mathbf{E}] + \mathbf{E}_{pos}
\end{equation}
其中，$\mathbf{x}_p^i$ 表示第 $i$ 个图像块，$\mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}$ 为线性投影矩阵，
$\mathbf{E}_{pos} \in \mathbb{R}^{N \times D}$ 为可学习的位置嵌入，$D$ 为特征维度。
%% 删除从这里结束
经过 Transformer 层的多头自注意力（MSA）机制处理后，
输入序列通过 $L$ 层 Transformer 编码器进行处理，每一层的特征更新遵循标准的自注意力（MSA）与多层感知机（MLP）机制：
\begin{equation}
\begin{aligned}
\mathbf{z}'_l &= \text{MSA}(\text{LN}(\mathbf{z}_{l-1})) + \mathbf{z}_{l-1} \\
\mathbf{z}_l &= \text{MLP}(\text{LN}(\mathbf{z}'_l)) + \mathbf{z}'_l, \quad l=1 \dots L
\end{aligned}
\end{equation}
其中 $\text{LN}(\cdot)$ 表示层归一化（Layer Normalization）。最终，编码器的最后一层输出即为全局语义标记 $\mathbf{T} = \mathbf{z}_L$。
输出序列化的标记集合 $\mathbf{T} = \{t_1, t_2, \dots, t_N\}$。这些 Token 保留了序列形式而未被重组为特征图，
它们通过自注意力机制充分交互，编码了图像的全局上下文信息。$\mathbf{T}$ 将直接被送入相对估计模块，
用于推断物体间的遮挡关系和整体场景结构。

2.多尺度空间特征 (Features $\mathbf{F}$)：
如图中下方路径所示，除了序列化的 Token，编码器还通过特征重组操作，从不同深度的 Transformer 层中提取出具有空间分辨率的特征图。
%% 公式3开始
为了恢复空间结构，我们从编码器的不同层级索引 $\mathcal{I} = \{l_1, l_2, l_3, l_4\}$ 中提取特征序列，并通过重组操作（Reassemble）将其映射回空间域：
\begin{equation}
f_i = \text{Conv}_{1 \times 1}(\text{Reshape}(\mathbf{z}_{l_i})), \quad i \in \{1, 2, 3, 4\}
\end{equation}
其中，$\text{Reshape}(\cdot)$ 操作将序列 $\mathbb{R}^{N \times D}$ 还原为 $\mathbb{R}^{\frac{H}{P_i} \times \frac{W}{P_i} \times D}$ 
的特征张量，$\text{Conv}_{1 \times 1}$ 用于调整通道维度以适配后续模块。
%% 公式3结束
本文将这些分层特征记为 $\mathbf{F} = \{f_1, f_2, f_3, f_4\}$，构成了包含从边缘纹理到抽象语义的特征块组合。
$\mathbf{F}$ 是绝对深度估计模块的主要输入之一。在本文提出的方法中，这些多尺度特征将被送入Xcs-SeedRegressor 和 Xcs-Attractor模块，
利用其中的坐标注意力机制（CoordAtt\cite{hou2021coordinate}）进一步挖掘空间位置线索，从而实现对绝对尺度的精确恢复。

% 在特征提取阶段，系统采用统一的共享编码器（Shared Encoder）作为底层基座。
% 对于输入的单幅 RGB 图像 $I \in \mathbb{R}^{3 \times H \times W}$，编码器通过分层表征学习提取出具有异构物理意义的特征。
% 如公式 \ref{eq:encoder} 所示：
% \begin{equation}
%     \{T, F\} = \text{Encoder}(I)
%     \label{eq:encoder}
% \end{equation}
% 其中，$T$ 为蕴含全局几何上下文的标记向量（Tokens）；$F = \{f_1, f_2, f_3, f_4\}$ 为多层级空间特征块（Feature Blocks），
% 对应于代码实现中不同层级的瓶颈层特征与解码块输出，承载了丰富的局部空间细节。

% 这种多路输出设计实现了任务驱动的特征分配逻辑。第一阶段相对估计分支利用标记向量 $T$ 构建场景的宏观拓扑，
% 确保深度图在几何序关系与物体边缘上的一致性；第二阶段度量映射分支则通过级联的投影器（Projector）处理特征块 $F$，
% 并利用局部特征块引导深度分桶（Bins）的自适应动态调整。

% 相较于传统的端到端直接回归范式，这种解构设计能够有效缓解单目深度估计的不适定性（Ill-posedness）。
% 通过在特征提取阶段将语义先验与尺度特征进行分离，模型能够有效抑制由于相机内参波动或异质场景切换带来的尺度漂移现象。这种解耦机制确保了模型在恢复物理尺度的同时，
% 能够最大限度保留基础模型所提供的强泛化几何结构，从而兼顾了预测结果的数值准确性与空间合理性。

\subsection{相对几何结构感知分支}

在单目深度估计任务中，建立稳健的几何拓扑关系是恢复绝对度量维度的前提。
本章所设计的相对几何结构感知分支旨在不考虑绝对物理尺寸的情况下，利用大规模预训练模型捕获的深层语义特征，
重构场景内物体的相对位置关系。该分支的输入为共享编码器输出的全局标记向量（Tokens）$\mathbf{T}$，
如公式 \ref{eq:rel_branch} 所示：
\begin{equation}
    D_{rel} = \Psi_{\text{rel}}(\mathbf{T})
    \label{eq:rel_branch}
\end{equation}
其中，$\Psi_{\text{rel}}$ 表示相对深度估计映射函数。

标记向量 $\mathbf{T}$ 进入相对估计模块后，首先通过多层感知机（MLP）进行维度对齐与特征压缩，
随后进入自注意力机制（Self-Attention）层进行长程依赖建模。自注意力机制的引入使得模型能够摆脱局部感受野的限制，
通过计算全图范围内的特征相关性，实现对场景宏观布局的深度理解。
这种全局建模能力对于识别复杂的物体间拓扑关系、处理大面积平坦区域以及跨越遮挡边界获取一致的几何先验至关重要。

该分支最终输出一张尺度不变的相对深度图 $D_{rel}$。该图能够精确捕获物体的轮廓细节、场景平面的斜率以及物体间的相互遮挡序关系。
尽管其数值不具备物理含义，但 $D_{rel}$ 提供的几何稳定性为下一阶段的尺度适配任务奠定了坚实基础。
通过将深度估计任务的第一阶段聚焦于几何拓扑的构建，模型能够显著增强在非约束场景下的泛化能力，
为后续实现精确的“结构-度量”映射提供可靠的几何度量锚点。

\subsection{绝对深度对齐分支}
度量深度估计模块（第二阶段）是整个感知框架中实现物理尺度恢复的核心组件。本章并未采用传统的直接数值回归范式，
而是引入了一种特征驱动的离散化期望回归机制。其核心逻辑在于将第一阶段捕获的稳健几何结构作为结构锚点，
结合编码器提取的多尺度空间特征，实现从相对几何序关系向物理度量空间的精准映射。

在处理流程上，该分支首先接收来自共享编码器的多尺度空间特征 $F$。通过级联的投影器（Projector）对特征进行非线性变换与空间维度对齐，
随后利用吸引器（Attractor）机制，根据特征块中蕴含的语义上下文对深度分桶（Bins）的质心分布进行自适应调整。
这种机制允许模型根据场景复杂度动态优化深度的搜索空间，从而在非约束环境下获得更细粒度的尺度表现。

为了最大化保留物体的边缘细节并抑制尺度漂移，本章引入了结构注入与融合机制。
第一阶段生成的相对深度图 $D_{rel}$ 经过双线性插值上采样后，与编码器末端的高维空间特征进行维度拼接（Concatenation），
构建出增强型的几何-尺度联合表征。如公式 \ref{eq:fusion} 所示：
\begin{equation}
    X_{fusion} = [F_{last} \oplus \text{up}(D_{rel})]
    \label{eq:fusion}
\end{equation}
其中，$\oplus$ 表示通道维度的拼接操作。

在最终的预测环节，模型利用条件对数二项式分布（Conditional Log Binomial）计算每个像素在各深度分桶上的概率响应。
最终的绝对度量深度 $D_{met}$ 是通过对所有分桶质心 $B$ 进行概率加权求和所得的离散期望值：
\begin{equation}
    D_{met} = \sum_{i=1}^{N} P_i \cdot B_i
    \label{eq:metric_output}
\end{equation}
相较于单一数值回归，这种基于概率分布的建模方式能够有效缓解单目估计中的尺度歧义性，
并在优化全局数值精度的同时，确保物体边缘与空间拓扑的一致性。

\section{实验结果与分析}
为全面验证本章所提方法的有效性与稳健性，本节在单目绝对深度估计领域两个极具代表性且富有挑战性的基准数据集上开展了详尽的实验。
首先，对室内场景数据集 NYU-Depth v2\cite{dataset_nyu}与室外自动驾驶数据集 KITTI\cite{dataset_kitti}的基本概况进行简要说明。
其次，明确绝对深度估计领域通用的性能评价指标，并详细介绍实验的软硬件环境与超参数设置。
最后，通过定性与定量的对比实验验证本方法相较于当前主流算法的领先性，
并结合消融实验深入探讨各核心模块对模型性能的贡献，旨在阐明本方法在复杂环境下恢复物理度量深度的优越性。

\subsection{数据集}
NYU-Depth v2 数据集是由纽约大学 Silberman 等研究人员通过 Microsoft Kinect 设备采集的大规模室内场景数据集，
主要针对的是室内场景理解、语义分割以及深度估计相关问题。
该数据集中共包含 1,449 张经过人工标注并对齐的 RGB 与深度图像（RGB-D）对，
场景涵盖了卧室、厨房、办公室、客厅等多种复杂的室内环境。数据集类别包括家具、电器、墙壁等共计 894 类物体。
由于其采用真实场景数据，因此包含了室内环境中常见的光照变化、物体遮挡以及复杂的空间布局，
为单目绝对深度估计的研究提供了重要的数据支撑。

NYU-Depth v2 数据集除了提供高精度的深度标注信息之外，还根据语义一致性提供了密集的像素级分割标签，
为计算机视觉系统在室内环境下的开发提供了全方位的数据支持。该数据集主要适合用于单目深度估计的训练与评估，
也可用于室内语义分割、目标检测等任务。同时，由于数据中自然包含了不同室内布局下的空间几何信息，
其已成为度量绝对深度估计领域最常用的基准测试集之一。

本文关注模型在室内复杂布局下的尺度恢复能力，因此采用的是 NYU-Depth v2 的官方标准划分方式。
实验选取了由原始视频序列预处理得到的约 50,000 张图像对作为训练数据，以确保模型能够充分学习室内场景的特征分布；
并严格采用官方预定义的 654 张测试图像作为最终的性能评估集。为了准确测试模型的跨场景泛化性能与尺度恢复精度，
实验过程中对深度真值进行了有效的范围截断与对齐处理，确保模型能够在多样化的室内闭环环境下保持较高的估计效果。

\subsubsection{KITTI 数据集}

KITTI（Karlsruhe In\-sti\-tute of Tech\-nol\-o\-gy and Toy\-o\-ta Tech\-no\-log\-i\-cal In\-sti\-tute at Chi\-ca\-go）
数据集是目前国际上最受认可的室外自动驾驶场景计算机视觉算法评测基准之一。
该数据集由德国卡尔斯鲁厄理工学院和芝加哥丰田技术研究院联合创建，
利用配备了彩色/灰度摄像机、高精度旋转式激光扫描仪（LiDAR）以及 GPS/IMU 导航系统的车载平台，
在城市街区、乡村道路和高速公路等多样化的真实道路环境下采集而成。
在单目深度估计任务中，KITTI 数据集提供的激光雷达点云数据经投影对齐后，可作为度量深度估计的稀疏真值参考，
其覆盖范围通常可达 80 米。

在实验分析中，本文遵循单目深度估计领域的通行规范，采用了广泛认可的 Eigen 划分方式（Eigen Split）\cite{eigen2014}。
该划分逻辑由 Eigen 等人提出，通过对原始序列进行筛选，有效去除了冗余或传感器失效的帧，
最终形成了包含 23,488 对原始图像序列的训练集，以及 697 张具有高质量深度标注的测试图像。
由于激光雷达采集的原始点云具有稀疏性，本文在预处理阶段采用了反距离加权插值或官方提供的深度填充算法进行优化，以获得更为致密的监督信号。

针对室外自动驾驶环境下相机内参多样且光照变化剧烈的特点，KITTI 数据集为验证本方法在高速移动场景下的尺度对齐能力提供了绝佳的平台。
本文在实验中特别关注了模型对远景（如建筑、植被）与近景（如车辆、行人）的结构刻画能力。
通过在该数据集上的测试，能够有效评估本章提出的二阶段解构映射方法在应对室外大尺度空间变化时的鲁棒性与泛化性能。

\subsection{评价指标}

为了客观、全面地评估本文所提模型在单目绝对深度估计任务上的性能，本文在实验部分采用了深度估计领域公认的通用评价指标体系进行定量分析。
评价指标主要分为误差指标（Error Metrics）与准确度指标（Accuracy Metrics）两类。
令 $d_i$ 表示第 $i$ 个像素的深度真实值（Ground Truth），$\hat{d}_i$ 表示对应的模型预测值，$N$ 为参与评估的像素总数。

\noindent \textbf{（1）误差指标分析}

误差指标旨在衡量模型预测结果与真实值之间的数值偏差。首先，平均相对误差（Abs Rel）反映了预测深度相对于真实深度的平均偏移比例，其计算公式如式（\ref{eq:absrel}）所示：
\begin{equation}
    \text{Abs Rel} = \frac{1}{N} \sum_{i=1}^{N} \frac{|d_i - \hat{d}_i|}{d_i}
    \label{eq:absrel}
\end{equation}
该指标对全量程范围内的误差具有较好的均衡评价能力。其次，为了评估模型预测的稳定性，本文引入了均方根误差（RMSE）。由于 RMSE 采用了平方运算，其对预测结果中的异常值和较大偏差极度敏感，计算公式如式（\ref{eq:rmse}）所示：
\begin{equation}
    \text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (d_i - \hat{d}_i)^2}
    \label{eq:rmse}
\end{equation}
此外，对数平均误差（$\log_{10}$）通过将深度值映射到对数空间进行计算，能够有效缩小远近景数值量级的差异，从而更客观地评价模型在不同距离下的线性一致性：
\begin{equation}
    \log_{10} = \frac{1}{N} \sum_{i=1}^{N} |\log_{10} d_i - \log_{10} \hat{d}_i|
    \label{eq:log10}
\end{equation}
对于上述三项误差指标而言，其数值越小，通常代表模型的绝对深度还原精度越高。

\noindent \textbf{（2）准确度指标分析}

准确度指标通过统计满足特定阈值条件的像素占比来衡量预测图的贴合程度。本文采用阈值准确度 $\delta_n$ 作为评价标准，其定义如式（\ref{eq:delta}）所示：
\begin{equation}
    \delta_n = \text{percentage of } d_i \text{ s.t. } \max \left( \frac{d_i}{\hat{d}_i}, \frac{\hat{d}_i}{d_i} \right) < 1.25^n
    \label{eq:delta}
\end{equation}
其中，$n \in \{1, 2, 3\}$ 分别对应三个不同严苛程度的阈值。在实验结果分析中，$\delta_1$（即阈值小于 1.25）是衡量深度估计精度的最关键指标，反映了模型实现高精度像素级还原的能力；而 $\delta_2$ 和 $\delta_3$ 则代表了在中等及较宽容许误差下的准确率。
对于此类准确度指标，数值越大代表模型的性能表现越优异。

