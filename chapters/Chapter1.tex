% !TEX root = ../main.tex
\chapter{绪论}

\section{研究意义与背景}
深度估计作为三维计算机视觉领域的核心基础问题，旨在从二维图像中重建场景的几何特征，在过去十余年中经历了从手工特征建模到深度学习驱动的范式变迁\cite{Eigen,dpt}。
随着传感器技术与计算能力的提升，深度估计已成为智能无人系统实现环境感知与定位导航的关键技术：
在自动驾驶中，它为障碍物检测与路径规划提供必要的距离信息；在无人机技术中，它是实现自主避障与三维测绘的基础；在具身智能领域，深度估计则赋予了智能体感知空间结构并进行物理交互的能力。

从技术演进的维度看，单目深度估计的研究历程主要经历了三个阶段。初期阶段主要依赖手工设计的几何特征与先验假设 \cite{make3d}，
通过概率图模型整合图像的底层信息，但在复杂场景下的建模鲁棒性较差。
中期阶段随着卷积神经网络（CNN）的兴起，研究重心转向端到端的监督学习 \cite{dorn}，通过多尺度网络架构显著提升了像素级的预测精度。
现阶段则迈入了以大模型和多任务迁移为特征的新时期，Vision Transformer (ViT) 等架构的应用 \cite{dpt} 极大增强了模型对全局几何上下文的理解。

然而，现有的深度估计方法在非约束场景下的表现仍面临严峻挑战。由于单目深度估计本质上是一个不适定问题，存在固有的比例模糊性\cite{Eigen,uncertainty}，
神经网络往往倾向于通过学习训练集中的统计偏见（如物体位置与深度的相关性）来“走捷径”，而非真正理解场景的物理几何。这种策略导致模型过度拟合了特定数据集（如 KITTI 或 NYU Depth V2）的成像特性，使其对相机内参及拍摄视角具有极强的依赖性。一旦应用于光照剧变、极端天气或异质场景，由于领域鸿沟的存在，模型的预测精度往往会出现断崖式下降，限制了其在跨平台部署时的零样本迁移能力。

针对上述挑战，学术界开始探索一种新的研究范式：利用强泛化性的相对深度信息辅助绝对深度的估计\cite{midas}。 
相对深度虽然不具备物理单位，但其能通过海量异质数据的预训练，捕捉到稳健的几何拓扑关系与遮挡先验，展现出极佳的场景鲁棒性。

本文认为，融合相对深度的几何先验优势与绝对深度的尺度特性，是实现跨场景稳定感知的关键路径。 
通过设计一种能够解耦几何结构与物理尺度的预测框架，利用大模型提取的全局几何一致性来约束局部尺度恢复，可显著降低模型对特定相机内参的耦合。
这种方法旨在打破单目深度估计在未知场景下的精度瓶颈，为无人系统在复杂、全天候环境下的高精度感知提供新的理论支撑与技术方案。
\begin{figure}[!ht]    % [htbp] 表示建议 LaTeX 放置图片的位置（Here, Top, Bottom, Page）
    \centering          % 使图片居中
    \includegraphics[width=0.8\textwidth]{figures/ch1-drone.jpg} % 填入图片文件名（不带后缀也可以），width 指定宽度
    \caption{无人机} % 图片下方的标题
    \label{fig:ch1_drone} % 图片的唯一标签，用于正文引用
\end{figure}

\section{国内外研究现状}
\subsection{相对深度估计研究现状}
单目深度估计根据输出表征的不同，通常可分为绝对深度估计与相对深度估计。
绝对深度估计旨在恢复具有明确物理尺度的度量距离，但在非约束场景下常面临严峻的尺度模糊性挑战；
而相对深度估计则侧重于建模场景内物体的几何序关系，在复杂环境及跨领域迁移中展现出更强的鲁棒性。
相对深度特征的建模最早可追溯至 Saxena 等人\cite{make3d}的研究，虽然其核心目标是恢复场景的三维结构，
但该工作首次大规模利用马尔可夫随机场（Markov Random Field, MRF）来建模像素块间的空间相关性与相对位置关系，为后续相对深度概念的提出奠定了几何建模基础。
2016年，Chen 等人\cite{Chen2016}在《Single-Image Depth Perception in the Wild》中真正将“相对深度估计”确立为一个独立的研究命题。
该工作证明了深度信息可以脱离昂贵的传感器真值，通过人类标注的像素对排序先验（Ranking Perception）进行端到端训练，实现了在自然界复杂场景下的深度感知。
随后，Ranftl 等人\cite{midas}进一步完善了该范式，正式提出了跨数据集混合训练方案。
针对电影、激光雷达、虚拟游戏等异质数据集绝对单位不统一的问题，该研究通过尺度与平移不变的损失函数（Scale-and-shift-invariant Loss），
将多元数据统一在相对深度框架下进行联合训练，极大地提升了模型的零样本（Zero-shot）泛化能力。
尽管上述工作在几何结构的稳健性方面取得了显著突破，但由于相对深度输出缺乏真实的物理尺度信息（Metric Scale），导致其在自动驾驶、无人机避障等对绝对距离高度敏感的实际任务中应用受限。

\subsection{绝对深度估计研究现状}
绝对深度估计（Metric Depth Estimation）旨在直接建立图像特征与物理距离之间的回归映射。早期研究主要聚焦于如何在受限场景下提升度量精度。Eigen 等人\cite{eigen2014}首次利用多尺度卷积神经网络实现了端到端的深度回归，奠定了深度学习在该领域的基础。随后，为了解决连续值回归收敛困难的问题，Fu 等人\cite{dorn}提出了深度序数回归网络（DORN），通过间隔递增离散化策略将回归任务转化为有序分类任务，显著提升了模型在特定数据集上的绝对数值精度。

在真值获取方面，针对激光雷达数据稀疏且昂贵的挑战，Godard 等人\cite{monodepth2}提出了自监督学习范式，利用视频序列间的光度一致性作为约束，
极大拓宽了绝对深度估计的应用边界。然而，无论是全监督还是自监督方法，现有的绝对深度估计工作仍面临以下严峻挑战：

首先，模型对相机内参及场景分布存在严重的“尺度耦合”。
现有的绝对深度预测框架往往将场景几何结构与物理尺度混合建模，导致模型极易过拟合于特定相机的成像特性。
一旦测试环境的焦距、拍摄高度与训练集存在差异，预测结果便会产生剧烈的尺度偏移。

其次，现有工作对相对深度所蕴含的稳健几何先验利用不足。虽然相对深度在建模物体遮挡与空间拓扑方面具有天然优势，但多数绝对深度估计方法仍尝试从头学习像素级的数值映射，
而忽视了相对深度作为强力几何约束的潜力。
这导致现有模型在面对光照剧变或异质场景时，由于缺乏稳健的几何结构感知，其泛化能力始终无法满足全天候、全场景无人系统的部署需求。

\begin{table}[!ht] % H 表示强制固定在这里，依赖你加载的 float 包
  \centering
  \caption{基础三线表示例}
  \begin{tabular}{llll} % 四个 l 代表四列全部左对齐
    \toprule
    方法 & 参数 & 准确率 & 耗时 (s) \\
    \midrule
    Baseline  & 128  & 85.2\% & 12.5 \\
    Ours      & 256  & 92.4\% & 18.2 \\
    Improved  & 512  & 94.1\% & 25.0 \\
    \bottomrule
  \end{tabular}
\end{table}

% \subsection{废弃}
% 深度估计根据输出性质可分为绝对深度估计与相对深度估计。
% 绝对深度估计旨在恢复具有物理尺度的度量距离，但在跨场景应用中面临尺度模糊性挑战；相对深度估计则侧重于建模场景的几何序关系，展现出更强的环境鲁棒性。
% 相对深度估计这一概念首先出现在Saxena等人的工作中，其工作的目的旨在恢复3D结构，但他们首次大规模的利用了马尔可夫随机场（MRF）
% 来建模像素块之间的空间相关性和相对位置关系。这为后来“相对”的概念埋下了伏笔。
% wei等人真正将“相对深度估计”作为一个独立命题，并证明其可以脱离传感器真值进行训练。
% Ranftl等人正式提出跨数据集混合训练，由于不同数据集（电影、激光雷达、游戏）的绝对单位不同，他们通过数学转换，将所有数据集统一在“相对深度”框架下进行训练。
% 这些人的工作很好的提高了相对深度估计的泛化能力，然而因为相对深度估计无法提供尺度信息，因此实际价值有限。
