\chapter{基于相对深度引导的映射模块}
\section{引言}
随着具身智能技术的普及，单目深度估计（MDE）已成为自动驾驶与机器人感知的核心环节。按照表征形式，
MDE 可分为相对深度估计与绝对深度（Metric Depth）估计，后者因能恢复具备真实物理尺度的像素级距离，
成为系统从“视觉感知”迈向“物理交互”的关键桥梁。在避障规划、目标抓取及视觉 SLAM 尺度恢复等任务中，
绝对尺度信息提供了不可或缺的几何度量约束。
相比激光雷达等昂贵设备，单目绝对深度估计凭借低成本与高灵活性，已成为构建全场景无人系统感知的核心基石。

如前文所述，从单幅图像推算绝对深度本质上是一个典型的“不适定问题”（Ill-posed Problem）。 
目前主流的单目绝对深度估计多采用端到端的直接回归范式，这种“一步到位”的映射机制使得模型不可避免地陷入尺度信息与几何结构的强耦合困境。
在这种机制下，模型往往过度依赖训练集中的语义先验（如物体的平均尺寸）来推断距离，而非理解真正的投影几何关系。
这导致模型在面对异质场景或相机内参发生波动时，极易产生剧烈的尺度漂移（Scale Drift），并导致物体边缘与空间布局的一致性受损。

值得注意的是，现有的绝对深度估计工作大多倾向于构建从图像特征到物理数值的直接线性或非线性映射，
这种粗放的回归方式忽略了相对深度中蕴含的稳健几何拓扑约束。由于绝对深度真值（Ground Truth）通常由稀疏的激光雷达获取，
且覆盖场景有限，模型在拟合有限的数值分布时，容易丧失对物体间细粒度几何序关系的把握。这导致模型在处理复杂场景时，
虽然能在统计数值上逼近真值，但其三维空间结构常发生畸变，无法满足高精度导航对环境结构化的严苛要求。

然而，现有的绝对深度估计工作大多倾向于构建从图像到物理数值的直接映射，
这种“一步到位”的粗放回归方式往往忽略了相对深度中蕴含的稳健几何拓扑约束。这导致模型在处理复杂场景时，
虽然能在数值上逼近真值，但其空间结构常发生畸变，且极易受相机参数波动的干扰。

针对上述挑战，本章提出了一种融合相对深度特征先验的二阶段解构式绝对深度估计方法。
该方法的核心逻辑在于将绝对深度估计任务解构为“结构感知”与“尺度适配”两个独立且互补的环节。
第一阶段利用具有强泛化力的预训练基础模型提取尺度不变的相对几何特征；第二阶段则通过一个轻量化映射模块，
利用多尺度特征块（Feature Blocks）中丰富的上下文信息引导模型完成从相对空间到物理尺度的精准转换。
这种解耦设计不仅能最大限度保留基础模型在“野外”环境下的结构泛化力，
更通过特征驱动的映射机制实现了物理尺度的精确对齐，为构建高鲁棒性的绝对深度感知系统提供了新思路。

\section{模型总体结构设计}

% 感觉图片里字有点小了。另外 不知道图片中英文好不好
\begin{figure}[htbp]
    \centering
    \begin{adjustbox}{center} % 强制相对于页面中心对齐
        \includegraphics[width=1.2\textwidth]{figures/ch1-整体结构图v2.png}
    \end{adjustbox}
    \caption{单目深度估计任务解构与特征提取框架}
    \label{fig:model_arch}
\end{figure}

本章所设计的绝对深度估计模型采用了任务解构的二阶段架构，旨在充分利用预训练基础模型的几何结构泛化能力，
并通过轻量化的度量映射分支实现物理尺度的精准恢复。模型总体架构如图\ref{fig:model_arch}所示，主要由特征提取编码器（Encoder）、
相对深度估计模块（Relative Estimation Module）以及度量深度估计模块（Metric Estimation Module）三部分组成。


\subsection{特征编码器 (Feature Encoder)}
\label{subsec:encoder}

\begin{figure}[htbp]
    \centering
    \begin{adjustbox}{center} % 强制相对于页面中心对齐
        \includegraphics[width=1.0\textwidth]{figures/Encoder结构示意.png}
    \end{adjustbox}
    \caption{单目深度估计任务解构与特征提取框架}
    \label{fig:encoder_arch}
\end{figure}

在本节中，我们详细阐述模型的编码器结构。本文采用 Vision Transformer (ViT) 的变体作为骨干网络（Backbone），
旨在从输入图像中提取具有长距离依赖关系的全局上下文特征。与传统的卷积神经网络（CNN）随着网络加深逐步降低空间分辨率不同，
基于 Transformer 的编码器在所有阶段均保持恒定的 Token 序列长度，从而确保模型在每一层都拥有全局感受野。

\subsubsection{图像分块与嵌入 (Patch Embedding)}
给定输入图像 $I \in \mathbb{R}^{H \times W \times C}$，其中 $(H, W)$ 表示图像的空间分辨率，$C$ 为通道数。
首先，我们将图像 $I$ 划分为一系列固定大小的非重叠切片（Patch），记为 $x_p \in \mathbb{R}^{N \times (P^2 \cdot C)}$，
其中 $(P, P)$ 为每个切片的分辨率，$N = \frac{HW}{P^2}$ 为切片的总数。

如图 \ref{fig:encoder_arch}所示，通过一个可学习的线性投影层 $E$，将展平后的切片映射到潜在向量维度 $D$。
为了保留图像的空间位置信息，我们将可学习的位置编码 $E_{pos}$ 逐元素加到切片嵌入中。此外，我们在序列首部拼接了一个特殊的“Readout Token”（通常称为 Class Token），
用于聚合全局语义信息。编码器的初始输入 $z_0$ 可表示为：

\begin{equation}
z_0 = [x_{readout}; x_p^1 E; x_p^2 E; \dots; x_p^N E] + E_{pos},
\end{equation}

其中 $x_{readout} \in \mathbb{R}^D$ 表示用于全局特征聚合的 Readout Token。

\subsubsection{Transformer 层 (Transformer Layers)}
编码器由 $L$ 个堆叠的 Transformer 模块组成。每个模块包含两个主要子层：多头自注意力机制 (Multi-Head Self-Attention, MSA) 
和多层感知机 (Multi-Layer Perceptron, MLP)。在每个子层之前均应用层归一化 (Layer Normalization, LN)，并在每个子层之后采用残差连接。
第 $l$ 层的计算过程如下：

\begin{equation}
\begin{aligned}
z'_l &= \text{MSA}(\text{LN}(z_{l-1})) + z_{l-1}, \\
z_l &= \text{MLP}(\text{LN}(z'_l)) + z'_l,
\end{aligned}
\end{equation}

其中 $z_l$ 表示第 $l$ 层的输出表示。得益于自注意力机制，编码器能够从第一层开始即有效地建模图像中任意两个切片之间的长距离依赖关系。

\subsubsection{层级特征提取 (Hierarchical Feature Extraction)}
尽管 Transformer 编码器输出的 Token 序列维度始终为 $D$，但网络不同深度的层能够捕获不同层级的语义抽象。
为了适应后续密集预测任务的需求，我们采用了层级特征提取策略。如图所示，我们从预选的层（例如层索引 $\{l_1, l_2, l_3, l_4\}$）中提取 Token 表示，
作为多尺度特征输入，用于后续的重组 (Reassemble) 和融合 (Fusion) 模块。这些中间层特征兼顾了底层的纹理细节与高层的语义语境，对深度估计任务至关重要。

\subsection{特征重组模块 (Reassemble Module)}
\label{subsec:reassemble}

Vision Transformer 编码器输出的是一维的词元序列，缺乏传统卷积神经网络所需的二维空间结构。
为了将这些序列特征用于密集的像素级预测，我们需要将其恢复为类图像的特征图。
如图所示，我们在编码器和融合模块之间引入了重组 (Reassemble) 操作。

对于从编码器第 $l$ 层提取的输出词元序列 $z_l \in \mathbb{R}^{(N+1) \times D}$（包含 $N$ 个图像词元和 1 个 Readout 词元），
重组过程包含以下三个主要阶段：

\subsubsection{读取与空间还原 (Read and Reshape)}
首先，我们执行 \texttt{Read} 操作。由于 Readout 词元主要聚合全局信息而缺乏明确的空间对应关系，我们在重组阶段将其丢弃，仅保留 $N$ 个图像词元。
随后，利用 \texttt{Concatenate} 操作将这些一维词元按照其在原始图像中的空间位置重新排列。
假设切片大小为 $P \times P$，我们将序列 $z_l^{img} \in \mathbb{R}^{N \times D}$ 重塑为二维特征图 $f_l \in \mathbb{R}^{\frac{H}{P} \times \frac{W}{P} \times D}$。

\subsubsection{特征投影 (Projection)}
为了统一来自不同深度的特征维度，便于后续融合，我们对重塑后的特征图应用 \texttt{Project} 操作。
具体而言，使用一个 $1 \times 1$ 的卷积层将输入通道维度 $D$ 映射到固定的嵌入维度 $\hat{D}$（例如 256）：

\begin{equation}
\hat{f}_l = \text{Conv}_{1\times1}(f_l),
\end{equation}

其中 $\hat{f}_l \in \mathbb{R}^{\frac{H}{P} \times \frac{W}{P} \times \hat{D}}$。

\subsubsection{多尺度重采样 (Resample)}
为了构建特征金字塔，我们需要将特征图调整到目标分辨率。我们定义 \texttt{Resample} 操作，根据所需的输出比例 $s$（相对于输入图像的分辨率，如 $\frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{32}$），对 $\hat{f}_l$ 进行空间上的上采样或下采样。
\begin{itemize}
    \item 对于需要下采样的阶段（如 $s \ge P$），我们使用步长卷积 (Strided Convolution) 来降低空间分辨率。
    \item 对于需要上采样的阶段（如 $s < P$），我们使用转置卷积 (Transposed Convolution) 进行上采样。
\end{itemize}

经过上述处理，我们得到了一组多尺度的特征图 $\{F_{s_1}, F_{s_2}, F_{s_3}, F_{s_4}\}$，
它们将作为输入被送入后续的融合模块 (Fusion Module) 中。

\subsection{特征融合与上采样 (Feature Fusion and Upsampling)}
\label{subsec:fusion}

为了从多尺度特征中恢复精细的密集预测结果，我们需要将重组后的特征图进行逐级融合。如图 \ref{fig:encoder_arch} 右下侧所示，我们设计了一个特征融合模块 (Fusion Module)，旨在结合深层网络的高级语义信息与浅层网络的空间细节信息。

融合过程采用由深至浅（Coarse-to-Fine）的级联结构。对于每一个融合阶段，该模块接收两路输入：
\begin{itemize}
    \item \textbf{横向输入 (Lateral Input)}：来自当前层级 Reassemble 模块的特征图，记为 $R_s$。这部分特征通过残差连接引入，保留了当前分辨率下的特征细节。
    \item \textbf{纵向输入 (Vertical Input)}：来自上一级融合模块输出的特征图，记为 $M_{prev}$。这部分特征通过了上采样处理，携带了更抽象的全局语义。
\end{itemize}

具体融合流程包含以下步骤：

\subsubsection{残差卷积单元 (Residual Convolutional Units)}
首先，为了对齐特征表示并增强非线性表达能力，横向输入的特征图 $R_s$ 会经过一个残差卷积单元 (Residual Conv Unit)。
该单元包含两个 $3 \times 3$ 卷积层（带激活函数），并包含恒等映射路径。随后，我们将处理后的横向特征与纵向特征进行逐元素相加 (Element-wise Addition)：

\begin{equation}
M_{sum} = \text{ResConv}(R_s) + M_{prev}
\end{equation}

相加后的特征 $M_{sum}$ 会再次经过一个残差卷积单元进行特征平滑与整合。

\subsubsection{上采样与投影 (Upsampling and Projection)}
为了将融合后的特征传递至下一个更高分辨率的阶段，我们需要对特征图进行空间上采样。如图所示的Resample$_{0.5}$操作表示将空间分辨率扩大为输入的2倍（即尺度因子为 0.5 的逆操作）。采用双线性插值 (Bilinear Interpolation)来实现。

最后，通过Project层（通常为 $1 \times 1$ 卷积）将通道数调整为下一阶段所需的维度。经过多次级联融合后，最终的高分辨率特征图被送入预测头 (Prediction Head) 以生成最终的深度图。

\subsection{任务解构与特征提取}
为了从根本上解决单目绝对深度估计中由于投影歧义性导致的尺度与结构强耦合问题，
本文将深度恢复过程解构为“结构感知”与“尺度适配”两个解耦的子任务。
该设计旨在利用大规模预训练模型（如 DPT-BEiT 或 MiDaS 系列）的几何泛化能力，为绝对尺度的恢复提供稳健的拓扑约束。

如图\ref{fig:model_arch}左侧所示，编码器码器模块是整个深度估计框架的特征提取基石。给定输入 RGB 图像$I \in \mathbb{R}^{H \times W \times 3}$，
编码器的主要任务是将高维像素信息映射为包含语义的深层特征表示。
为了同时满足相对深度估计对全局结构的需求，以及度量深度恢复对局部细节的需求，
本文采用基于 Vision Transformer (ViT) \cite{dosovitskiy2021image}的架构（如 DPT/BEiT）作为主干网络。
与传统的卷积神经网络不同，Transformer 架构天然具备长距离建模能力，能够有效捕捉场景中的上下文依赖关系。
编码器在处理过程中生成两类关键的特征表示，分别服务于后续的两个并行分支。

%% 这一段公式和公式说明很多。其实显得有些啰嗦了。而且也不知道会不会影响查重。有必要的话还是删除一部分吧。
%% 我靠他妈的都不够字数。还是别删了 
1.全局语义标记 (Tokens $\mathbf{T}$)：
如图中上方路径所示，输入图像首先被切分为固定大小的图块（Patches），并被展平为序列向量。
%% 公式1开始
具体而言，对于分辨率为 $H \times W$ 的输入图像 $I$，本文将切分为 $N = HW/P^2$ 个尺寸为 $P \times P$ 的图块。
通过线性投影映射与位置编码相加，初始化输入序列 $\mathbf{z}_0$：
\begin{equation}
\mathbf{z}_0 = [\mathbf{x}_p^1\mathbf{E}; \mathbf{x}_p^2\mathbf{E}; \cdots; \mathbf{x}_p^N\mathbf{E}] + \mathbf{E}_{pos}
\end{equation}
其中，$\mathbf{x}_p^i$ 表示第 $i$ 个图像块，$\mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}$ 为线性投影矩阵，
$\mathbf{E}_{pos} \in \mathbb{R}^{N \times D}$ 为可学习的位置嵌入，$D$ 为特征维度。
%% 1结束
经过 Transformer 层的多头自注意力（MSA）机制处理后，
输入序列通过 $L$ 层 Transformer 编码器进行处理，每一层的特征更新遵循标准的自注意力（MSA）与多层感知机（MLP）机制：
\begin{equation}
\begin{aligned}
\mathbf{z}'_l &= \text{MSA}(\text{LN}(\mathbf{z}_{l-1})) + \mathbf{z}_{l-1} \\
\mathbf{z}_l &= \text{MLP}(\text{LN}(\mathbf{z}'_l)) + \mathbf{z}'_l, \quad l=1 \dots L
\end{aligned}
\end{equation}
其中 $\text{LN}(\cdot)$ 表示层归一化（Layer Normalization）。最终，编码器的最后一层输出即为全局语义标记 $\mathbf{T} = \mathbf{z}_L$。
输出序列化的标记集合 $\mathbf{T} = \{t_1, t_2, \dots, t_N\}$。这些 Token 保留了序列形式而未被重组为特征图，
它们通过自注意力机制充分交互，编码了图像的全局上下文信息。$\mathbf{T}$ 将直接被送入相对估计模块，
用于推断物体间的遮挡关系和整体场景结构。

2.多尺度空间特征 (Features $\mathbf{F}$)：
如图中下方路径所示，除了序列化的 Token，编码器还通过特征重组操作，从不同深度的 Transformer 层中提取出具有空间分辨率的特征图。
%% 公式3开始
为了恢复空间结构，本文从编码器的不同层级索引 $\mathcal{I} = \{l_1, l_2, l_3, l_4\}$ 中提取特征序列，并通过重组操作（Reassemble）将其映射回空间域：
\begin{equation}
f_i = \text{Conv}_{1 \times 1}(\text{Reshape}(\mathbf{z}_{l_i})), \quad i \in \{1, 2, 3, 4\}
\end{equation}
其中，$\text{Reshape}(\cdot)$ 操作将序列 $\mathbb{R}^{N \times D}$ 还原为 $\mathbb{R}^{\frac{H}{P_i} \times \frac{W}{P_i} \times D}$ 
的特征张量，$\text{Conv}_{1 \times 1}$ 用于调整通道维度以适配后续模块。
%% 公式3结束
本文将这些分层特征记为 $\mathbf{F} = \{f_1, f_2, f_3, f_4\}$，构成了包含从边缘纹理到抽象语义的特征块组合。
$\mathbf{F}$ 是绝对深度估计模块的主要输入之一。在本文提出的方法中，这些多尺度特征将被送入Xcs-SeedRegressor 和 Xcs-Attractor模块，
利用其中的坐标注意力机制（CoordAtt\cite{hou2021coordinate}）进一步挖掘空间位置线索，从而实现对绝对尺度的精确恢复。

% 在特征提取阶段，系统采用统一的共享编码器（Shared Encoder）作为底层基座。
% 对于输入的单幅 RGB 图像 $I \in \mathbb{R}^{3 \times H \times W}$，编码器通过分层表征学习提取出具有异构物理意义的特征。
% 如公式 \ref{eq:encoder} 所示：
% \begin{equation}
%     \{T, F\} = \text{Encoder}(I)
%     \label{eq:encoder}
% \end{equation}
% 其中，$T$ 为蕴含全局几何上下文的标记向量（Tokens）；$F = \{f_1, f_2, f_3, f_4\}$ 为多层级空间特征块（Feature Blocks），
% 对应于代码实现中不同层级的瓶颈层特征与解码块输出，承载了丰富的局部空间细节。

% 这种多路输出设计实现了任务驱动的特征分配逻辑。第一阶段相对估计分支利用标记向量 $T$ 构建场景的宏观拓扑，
% 确保深度图在几何序关系与物体边缘上的一致性；第二阶段度量映射分支则通过级联的投影器（Projector）处理特征块 $F$，
% 并利用局部特征块引导深度分桶（Bins）的自适应动态调整。

% 相较于传统的端到端直接回归范式，这种解构设计能够有效缓解单目深度估计的不适定性（Ill-posedness）。
% 通过在特征提取阶段将语义先验与尺度特征进行分离，模型能够有效抑制由于相机内参波动或异质场景切换带来的尺度漂移现象。这种解耦机制确保了模型在恢复物理尺度的同时，
% 能够最大限度保留基础模型所提供的强泛化几何结构，从而兼顾了预测结果的数值准确性与空间合理性。

\subsection{相对几何结构感知分支}

在单目深度估计任务中，建立稳健的几何拓扑关系是恢复绝对度量维度的前提。
本章所设计的相对几何结构感知分支旨在不考虑绝对物理尺寸的情况下，利用大规模预训练模型捕获的深层语义特征，
重构场景内物体的相对位置关系。该分支的输入为共享编码器输出的全局标记向量（Tokens）$\mathbf{T}$，
如公式 \ref{eq:rel_branch} 所示：
\begin{equation}
    D_{rel} = \Psi_{\text{rel}}(\mathbf{T})
    \label{eq:rel_branch}
\end{equation}
其中，$\Psi_{\text{rel}}$ 表示相对深度估计映射函数。

标记向量 $\mathbf{T}$ 进入相对估计模块后，首先通过多层感知机（MLP）进行维度对齐与特征压缩，
随后进入自注意力机制（Self-Attention）层进行长程依赖建模。自注意力机制的引入使得模型能够摆脱局部感受野的限制，
通过计算全图范围内的特征相关性，实现对场景宏观布局的深度理解。
这种全局建模能力对于识别复杂的物体间拓扑关系、处理大面积平坦区域以及跨越遮挡边界获取一致的几何先验至关重要。

该分支最终输出一张尺度不变的相对深度图 $D_{rel}$。该图能够精确捕获物体的轮廓细节、场景平面的斜率以及物体间的相互遮挡序关系。
尽管其数值不具备物理含义，但 $D_{rel}$ 提供的几何稳定性为下一阶段的尺度适配任务奠定了坚实基础。
通过将深度估计任务的第一阶段聚焦于几何拓扑的构建，模型能够显著增强在非约束场景下的泛化能力，
为后续实现精确的“结构-度量”映射提供可靠的几何度量锚点。

\subsection{绝对深度对齐分支}
度量深度估计模块（第二阶段）是整个感知框架中实现物理尺度恢复的核心组件。本章并未采用传统的直接数值回归范式，
而是引入了一种特征驱动的离散化期望回归机制。其核心逻辑在于将第一阶段捕获的稳健几何结构作为结构锚点，
结合编码器提取的多尺度空间特征，实现从相对几何序关系向物理度量空间的精准映射。

在处理流程上，该分支首先接收来自共享编码器的多尺度空间特征 $F$。通过级联的投影器（Projector）对特征进行非线性变换与空间维度对齐，
随后利用吸引器（Attractor）机制，根据特征块中蕴含的语义上下文对深度分桶（Bins）的质心分布进行自适应调整。
这种机制允许模型根据场景复杂度动态优化深度的搜索空间，从而在非约束环境下获得更细粒度的尺度表现。

为了最大化保留物体的边缘细节并抑制尺度漂移，本章引入了结构注入与融合机制。
第一阶段生成的相对深度图 $D_{rel}$ 经过双线性插值上采样后，与编码器末端的高维空间特征进行维度拼接（Concatenation），
构建出增强型的几何-尺度联合表征。如公式 \ref{eq:fusion} 所示：
\begin{equation}
    X_{fusion} = [F_{last} \oplus \text{up}(D_{rel})]
    \label{eq:fusion}
\end{equation}
其中，$\oplus$ 表示通道维度的拼接操作。

在最终的预测环节，模型利用条件对数二项式分布（Conditional Log Binomial）计算每个像素在各深度分桶上的概率响应。
最终的绝对度量深度 $D_{met}$ 是通过对所有分桶质心 $B$ 进行概率加权求和所得的离散期望值：
\begin{equation}
    D_{met} = \sum_{i=1}^{N} P_i \cdot B_i
    \label{eq:metric_output}
\end{equation}
相较于单一数值回归，这种基于概率分布的建模方式能够有效缓解单目估计中的尺度歧义性，
并在优化全局数值精度的同时，确保物体边缘与空间拓扑的一致性。

\section{实验结果与分析}
为全面验证本章所提方法的有效性与稳健性，本节在单目绝对深度估计领域两个极具代表性且富有挑战性的基准数据集上开展了详尽的实验。
首先，对室内场景数据集 NYU-Depth v2\cite{dataset_nyu}与室外自动驾驶数据集 KITTI\cite{dataset_kitti}的基本概况进行简要说明。
其次，明确绝对深度估计领域通用的性能评价指标，并详细介绍实验的软硬件环境与超参数设置。
最后，通过定性与定量的对比实验验证本方法相较于当前主流算法的领先性，
并结合消融实验深入探讨各核心模块对模型性能的贡献，旨在阐明本方法在复杂环境下恢复物理度量深度的优越性。

\subsection{数据集}
NYU-Depth v2 数据集是由纽约大学 Silberman 等研究人员通过 Microsoft Kinect 设备采集的大规模室内场景数据集，
主要针对的是室内场景理解、语义分割以及深度估计相关问题。
该数据集中共包含 1,449 张经过人工标注并对齐的 RGB 与深度图像（RGB-D）对，
场景涵盖了卧室、厨房、办公室、客厅等多种复杂的室内环境。数据集类别包括家具、电器、墙壁等共计 894 类物体。
由于其采用真实场景数据，因此包含了室内环境中常见的光照变化、物体遮挡以及复杂的空间布局，
为单目绝对深度估计的研究提供了重要的数据支撑。

NYU-Depth v2 数据集除了提供高精度的深度标注信息之外，还根据语义一致性提供了密集的像素级分割标签，
为计算机视觉系统在室内环境下的开发提供了全方位的数据支持。该数据集主要适合用于单目深度估计的训练与评估，
也可用于室内语义分割、目标检测等任务。同时，由于数据中自然包含了不同室内布局下的空间几何信息，
其已成为度量绝对深度估计领域最常用的基准测试集之一。

本文关注模型在室内复杂布局下的尺度恢复能力，因此采用的是 NYU-Depth v2 的官方标准划分方式。
实验选取了由原始视频序列预处理得到的约 50,000 张图像对作为训练数据，以确保模型能够充分学习室内场景的特征分布；
并严格采用官方预定义的 654 张测试图像作为最终的性能评估集。为了准确测试模型的跨场景泛化性能与尺度恢复精度，
实验过程中对深度真值进行了有效的范围截断与对齐处理，确保模型能够在多样化的室内闭环环境下保持较高的估计效果。

\subsubsection{KITTI 数据集}

KITTI（Karlsruhe In\-sti\-tute of Tech\-nol\-o\-gy and Toy\-o\-ta Tech\-no\-log\-i\-cal In\-sti\-tute at Chi\-ca\-go）
数据集是目前国际上最受认可的室外自动驾驶场景计算机视觉算法评测基准之一。
该数据集由德国卡尔斯鲁厄理工学院和芝加哥丰田技术研究院联合创建，
利用配备了彩色/灰度摄像机、高精度旋转式激光扫描仪（LiDAR）以及 GPS/IMU 导航系统的车载平台，
在城市街区、乡村道路和高速公路等多样化的真实道路环境下采集而成。
在单目深度估计任务中，KITTI 数据集提供的激光雷达点云数据经投影对齐后，可作为度量深度估计的稀疏真值参考，
其覆盖范围通常可达 80 米。

在实验分析中，本文遵循单目深度估计领域的通行规范，采用了广泛认可的 Eigen 划分方式（Eigen Split）\cite{eigen2014}。
该划分逻辑由 Eigen 等人提出，通过对原始序列进行筛选，有效去除了冗余或传感器失效的帧，
最终形成了包含 23,488 对原始图像序列的训练集，以及 697 张具有高质量深度标注的测试图像。
由于激光雷达采集的原始点云具有稀疏性，本文在预处理阶段采用了反距离加权插值或官方提供的深度填充算法进行优化，以获得更为致密的监督信号。

针对室外自动驾驶环境下相机内参多样且光照变化剧烈的特点，KITTI 数据集为验证本方法在高速移动场景下的尺度对齐能力提供了绝佳的平台。
本文在实验中特别关注了模型对远景（如建筑、植被）与近景（如车辆、行人）的结构刻画能力。
通过在该数据集上的测试，能够有效评估本章提出的二阶段解构映射方法在应对室外大尺度空间变化时的鲁棒性与泛化性能。

\subsection{评价指标}

为了客观、全面地评估本文所提模型在单目绝对深度估计任务上的性能，本文在实验部分采用了深度估计领域公认的通用评价指标体系进行定量分析。
评价指标主要分为误差指标（Error Metrics）与准确度指标（Accuracy Metrics）两类。
令 $d_i$ 表示第 $i$ 个像素的深度真实值（Ground Truth），$\hat{d}_i$ 表示对应的模型预测值，$N$ 为参与评估的像素总数。

\noindent \textbf{（1）误差指标分析}

误差指标旨在衡量模型预测结果与真实值之间的数值偏差。首先，平均相对误差（Abs Rel）反映了预测深度相对于真实深度的平均偏移比例，其计算公式如式（\ref{eq:absrel}）所示：
\begin{equation}
    \text{Abs Rel} = \frac{1}{N} \sum_{i=1}^{N} \frac{|d_i - \hat{d}_i|}{d_i}
    \label{eq:absrel}
\end{equation}
该指标对全量程范围内的误差具有较好的均衡评价能力。其次，为了评估模型预测的稳定性，本文引入了均方根误差（RMSE）。由于 RMSE 采用了平方运算，其对预测结果中的异常值和较大偏差极度敏感，计算公式如式（\ref{eq:rmse}）所示：
\begin{equation}
    \text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (d_i - \hat{d}_i)^2}
    \label{eq:rmse}
\end{equation}
此外，对数平均误差（$\log_{10}$）通过将深度值映射到对数空间进行计算，能够有效缩小远近景数值量级的差异，从而更客观地评价模型在不同距离下的线性一致性：
\begin{equation}
    \log_{10} = \frac{1}{N} \sum_{i=1}^{N} |\log_{10} d_i - \log_{10} \hat{d}_i|
    \label{eq:log10}
\end{equation}
对于上述三项误差指标而言，其数值越小，通常代表模型的绝对深度还原精度越高。

\noindent \textbf{（2）准确度指标分析}

准确度指标通过统计满足特定阈值条件的像素占比来衡量预测图的贴合程度。本文采用阈值准确度 $\delta_n$ 作为评价标准，其定义如式（\ref{eq:delta}）所示：
\begin{equation}
    \delta_n = \text{percentage of } d_i \text{ s.t. } \max \left( \frac{d_i}{\hat{d}_i}, \frac{\hat{d}_i}{d_i} \right) < 1.25^n
    \label{eq:delta}
\end{equation}
其中，$n \in \{1, 2, 3\}$ 分别对应三个不同严苛程度的阈值。在实验结果分析中，$\delta_1$（即阈值小于 1.25）是衡量深度估计精度的最关键指标，反映了模型实现高精度像素级还原的能力；而 $\delta_2$ 和 $\delta_3$ 则代表了在中等及较宽容许误差下的准确率。
对于此类准确度指标，数值越大代表模型的性能表现越优异。

